{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TildeOpen-30b on Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/tilde-test/blob/main/TildeOpen_30b_Colab.ipynb)\n",
        "\n",
        "This notebook allows you to run the **TildeOpen-30b** model on Google Colab with free GPU access!\n",
        "\n",
        "## About TildeOpen-30b\n",
        "- üåç **30B parameter model** supporting **34 European languages**\n",
        "- üîì **Open Source** (CC-BY-4.0 license)\n",
        "- ‚ö° **Optimized** for Nordic and Eastern European languages\n",
        "- üèóÔ∏è **Enterprise Ready** - trained on LUMI supercomputer\n",
        "\n",
        "## Setup Instructions\n",
        "1. **Enable GPU**: Go to `Runtime` > `Change runtime type` > Select `GPU` (T4 or better)\n",
        "2. **Run all cells** in order\n",
        "3. **Start chatting** with the model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üöÄ Setup Environment\n",
        "# @markdown Run this cell first to install dependencies and check GPU availability\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"üîß Installing dependencies...\")\n",
        "\n",
        "# Install required packages\n",
        "packages = [\n",
        "    \"torch>=2.0.0\",\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"accelerate>=0.20.0\",\n",
        "    \"safetensors>=0.3.0\",\n",
        "    \"sentencepiece>=0.1.99\",\n",
        "    \"protobuf>=3.20.0\",\n",
        "    \"bitsandbytes>=0.41.0\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    print(f\"Installing {package}...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"\\nüéÆ CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üì± GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected. Please enable GPU in Runtime > Change runtime type\")\n",
        "\n",
        "print(\"\\nüéâ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üì• Load TildeOpen-30b Model\n",
        "# @markdown This will load the model with optimizations for Colab's GPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "class ColabTildeModel:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"TildeAI/TildeOpen-30b\"\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "    def load_model(self, use_4bit=True):\n",
        "        \"\"\"Load the model with Colab-optimized settings.\"\"\"\n",
        "        try:\n",
        "            print(\"üì• Loading tokenizer...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_name,\n",
        "                use_fast=False  # Required for this model\n",
        "            )\n",
        "            print(\"‚úÖ Tokenizer loaded!\")\n",
        "            \n",
        "            print(\"üì• Loading model with quantization...\")\n",
        "            \n",
        "            # Configure quantization for Colab's limited memory\n",
        "            if use_4bit and torch.cuda.is_available():\n",
        "                print(\"üîß Using 4-bit quantization for optimal memory usage\")\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\"\n",
        "                )\n",
        "                \n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    quantization_config=quantization_config,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    low_cpu_mem_usage=True\n",
        "                )\n",
        "            else:\n",
        "                print(\"üîß Loading without quantization\")\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    torch_dtype=torch.bfloat16,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    low_cpu_mem_usage=True\n",
        "                )\n",
        "            \n",
        "            print(\"‚úÖ Model loaded successfully!\")\n",
        "            \n",
        "            # Show memory usage\n",
        "            if torch.cuda.is_available():\n",
        "                memory_used = torch.cuda.memory_allocated() / 1024**3\n",
        "                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "                print(f\"üìä GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB used\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def generate_text(self, prompt, max_new_tokens=256, temperature=0.7):\n",
        "        \"\"\"Generate text using the loaded model.\"\"\"\n",
        "        if not self.model or not self.tokenizer:\n",
        "            raise RuntimeError(\"Model not loaded!\")\n",
        "        \n",
        "        try:\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True,\n",
        "                    repetition_penalty=1.1,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    use_cache=True\n",
        "                )\n",
        "            \n",
        "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            response = generated_text[len(prompt):].strip()\n",
        "            \n",
        "            # Clean up GPU memory\n",
        "            del outputs\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during generation: {e}\")\n",
        "            return None\n",
        "\n",
        "# Initialize and load the model\n",
        "print(\"üöÄ Initializing TildeOpen-30b for Google Colab...\")\n",
        "model = ColabTildeModel()\n",
        "\n",
        "if model.load_model():\n",
        "    print(\"\\nüéâ Model ready! You can now generate text in the next cell.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Failed to load model. Check GPU availability and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üß™ Test the Model\n",
        "# @markdown Run this cell to test the model with sample prompts\n",
        "\n",
        "test_prompts = [\n",
        "    \"Hello, I am TildeOpen and I can help you with\",\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Bonjour, je suis un mod√®le de langage qui peut\",  # French\n",
        "    \"–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, —è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç\"  # Russian\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing TildeOpen-30b with multilingual prompts...\\n\")\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"Test {i}: {prompt}\")\n",
        "    response = model.generate_text(prompt, max_new_tokens=50)\n",
        "    if response:\n",
        "        print(f\"Response: {response}\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to generate response\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"‚úÖ Model testing complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üí¨ Interactive Chat\n",
        "# @markdown Chat with TildeOpen-30b! Try different languages.\n",
        "\n",
        "def chat_with_model():\n",
        "    \"\"\"Interactive chat function for Colab.\"\"\"\n",
        "    print(\"üîÑ Starting chat with TildeOpen-30b\")\n",
        "    print(\"üí° Try different languages - this model supports 34 European languages!\")\n",
        "    print(\"üí° Type 'quit' to end the conversation\")\n",
        "    print(\"üí° Examples:\")\n",
        "    print(\"   - How are you today?\")\n",
        "    print(\"   - Comment allez-vous? (French)\")\n",
        "    print(\"   - –ö–∞–∫ –¥–µ–ª–∞? (Russian)\")\n",
        "    print(\"   - ¬øC√≥mo est√°s? (Spanish)\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    conversation_history = \"\"\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nüë§ You: \").strip()\n",
        "            \n",
        "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"üëã Goodbye!\")\n",
        "                break\n",
        "            \n",
        "            if user_input.lower() == 'clear':\n",
        "                conversation_history = \"\"\n",
        "                print(\"üßπ Conversation cleared!\")\n",
        "                continue\n",
        "            \n",
        "            if not user_input:\n",
        "                continue\n",
        "            \n",
        "            # Build prompt with conversation context\n",
        "            if conversation_history:\n",
        "                prompt = f\"{conversation_history}\\nHuman: {user_input}\\nAssistant:\"\n",
        "            else:\n",
        "                prompt = f\"Human: {user_input}\\nAssistant:\"\n",
        "            \n",
        "            # Keep conversation manageable\n",
        "            if len(prompt) > 1000:\n",
        "                lines = prompt.split('\\n')\n",
        "                prompt = '\\n'.join(lines[-6:])\n",
        "            \n",
        "            print(\"ü§ñ Generating response...\")\n",
        "            response = model.generate_text(prompt, max_new_tokens=200, temperature=0.8)\n",
        "            \n",
        "            if response:\n",
        "                print(f\"\\nü§ñ TildeOpen: {response}\")\n",
        "                conversation_history = f\"{prompt} {response}\"\n",
        "            else:\n",
        "                print(\"‚ùå Failed to generate response\")\n",
        "        \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nüëã Chat interrupted. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# Start the interactive chat\n",
        "chat_with_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Additional Information\n",
        "\n",
        "### Supported Languages (34 total)\n",
        "Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Hungarian, Icelandic, Irish, Italian, Latvian, Lithuanian, Macedonian, Maltese, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovene, Spanish, Swedish, Turkish, Ukrainian, and more.\n",
        "\n",
        "### Model Details\n",
        "- **Parameters**: 30 billion\n",
        "- **Training Data**: 2 trillion tokens\n",
        "- **License**: CC-BY-4.0\n",
        "- **Specialty**: Nordic and Eastern European languages\n",
        "\n",
        "### Performance Tips\n",
        "- Use shorter prompts for faster responses\n",
        "- The model performs best on European languages\n",
        "- Adjust `temperature` (0.1-1.0) to control creativity\n",
        "- Use `max_new_tokens` to control response length\n",
        "\n",
        "### Links\n",
        "- [Model on Hugging Face](https://huggingface.co/TildeAI/TildeOpen-30b)\n",
        "- [Tilde.ai](https://tilde.ai/tildeopen-llm/)\n",
        "- [GitHub Repository](https://github.com/YOUR_USERNAME/tilde-test)\n",
        "\n",
        "---\n",
        "**Happy experimenting with TildeOpen-30b! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
