#!/usr/bin/env python3
"""
Minimal TildeOpen-30b Interface
Uses streaming and minimal caching to reduce storage requirements.
Loads only essential model components and streams weights as needed.
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings
import tempfile
import os

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)

class MinimalTildeModel:
    def __init__(self, model_name="TildeAI/TildeOpen-30b"):
        """Initialize with minimal storage footprint."""
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        
        # Use a temporary directory that will be cleaned up
        self.temp_cache = tempfile.mkdtemp(prefix="tilde_cache_")
        print(f"üöÄ Minimal TildeOpen-30b Interface")
        print(f"üìÅ Temporary cache: {self.temp_cache}")
    
    def load_tokenizer_only(self):
        """Load only the tokenizer first (small download)."""
        try:
            print("üì• Loading tokenizer (minimal download)...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                use_fast=False,
                cache_dir=self.temp_cache
            )
            print("‚úÖ Tokenizer loaded successfully!")
            return True
        except Exception as e:
            print(f"‚ùå Failed to load tokenizer: {e}")
            return False
    
    def estimate_model_size(self):
        """Provide information about model requirements."""
        print("\nüìä TildeOpen-30b Model Information:")
        print("‚Ä¢ Parameters: ~30 billion")
        print("‚Ä¢ Model size: ~60GB (full precision)")
        print("‚Ä¢ RAM required: 32GB+ (minimum)")
        print("‚Ä¢ Recommended: GPU with 24GB+ VRAM")
        
        print(f"\nüíæ Your system:")
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"‚Ä¢ GPU: {torch.cuda.get_device_name()}")
            print(f"‚Ä¢ GPU Memory: {gpu_memory:.1f}GB")
            
            if gpu_memory >= 24:
                print("‚úÖ Sufficient GPU memory for model loading")
                return "gpu_good"
            elif gpu_memory >= 12:
                print("‚ö†Ô∏è  Limited GPU memory - quantization recommended")
                return "gpu_limited"
            else:
                print("‚ùå Insufficient GPU memory")
                return "gpu_insufficient"
        else:
            print("‚Ä¢ GPU: Not available")
            print("‚ùå CPU-only inference will be extremely slow")
            return "cpu_only"
    
    def test_tokenizer(self):
        """Test the tokenizer with sample text."""
        if not self.tokenizer:
            print("‚ùå Tokenizer not loaded")
            return False
        
        print("\nüß™ Testing tokenizer with multilingual samples...")
        
        test_texts = [
            "Hello, how are you?",  # English
            "Bonjour, comment allez-vous?",  # French
            "Hola, ¬øc√≥mo est√°s?",  # Spanish
            "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –∫–∞–∫ –¥–µ–ª–∞?",  # Russian
            "Witaj, jak siƒô masz?",  # Polish
        ]
        
        for text in test_texts:
            try:
                tokens = self.tokenizer.encode(text)
                decoded = self.tokenizer.decode(tokens)
                print(f"‚úÖ {text} ‚Üí {len(tokens)} tokens ‚Üí {decoded}")
            except Exception as e:
                print(f"‚ùå Failed to tokenize: {text} - {e}")
                return False
        
        return True
    
    def demo_without_model(self):
        """Demonstrate capabilities without loading the full model."""
        print("\nüéØ TildeOpen-30b Capabilities Demo (Tokenizer Only)")
        print("=" * 60)
        
        if not self.test_tokenizer():
            return False
        
        print(f"\nüåç Supported Languages:")
        languages = [
            "English", "German", "French", "Polish", "Russian", "Italian", 
            "Portuguese", "Czech", "Dutch", "Spanish", "Finnish", "Turkish",
            "Hungarian", "Bulgarian", "Croatian", "Danish", "Estonian",
            "Latvian", "Lithuanian", "Norwegian", "Romanian", "Serbian",
            "Slovak", "Slovene", "Swedish", "Ukrainian", "and more..."
        ]
        
        for i, lang in enumerate(languages):
            if i < len(languages) - 1:
                print(f"‚Ä¢ {lang}")
            else:
                print(f"‚Ä¢ {lang}")
        
        print(f"\nüìà Model Performance Highlights:")
        print("‚Ä¢ Optimized for Nordic and Eastern European languages")
        print("‚Ä¢ Trained on 2 trillion tokens")
        print("‚Ä¢ Equitable tokenizer for fair language representation")
        print("‚Ä¢ State-of-the-art performance on underrepresented languages")
        
        return True
    
    def interactive_tokenizer_demo(self):
        """Interactive demo using just the tokenizer."""
        print("\nüîÑ Interactive Tokenizer Demo")
        print("üí° Type text to see how TildeOpen tokenizes it")
        print("üí° Try different languages!")
        print("üí° Type 'quit' to exit")
        print("-" * 50)
        
        while True:
            try:
                user_input = input("\nüìù Enter text: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not user_input:
                    continue
                
                # Tokenize and analyze
                tokens = self.tokenizer.encode(user_input)
                decoded = self.tokenizer.decode(tokens, skip_special_tokens=True)
                
                print(f"üìä Analysis:")
                print(f"   Original: {user_input}")
                print(f"   Tokens: {len(tokens)}")
                print(f"   Token IDs: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
                print(f"   Decoded: {decoded}")
                print(f"   Efficiency: {len(user_input)/len(tokens):.2f} chars/token")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")
        
        print("üëã Demo ended!")
    
    def show_model_loading_options(self):
        """Show different options for loading the full model."""
        system_capability = self.estimate_model_size()
        
        print(f"\nüöÄ Model Loading Options:")
        print("=" * 40)
        
        if system_capability == "gpu_good":
            print("1. üéØ RECOMMENDED: Standard Loading")
            print("   python3 tilde_model.py")
            print("   ‚Ä¢ Full model capabilities")
            print("   ‚Ä¢ Best performance")
            
            print("\n2. üíæ Memory Optimized:")
            print("   python3 tilde_lightweight.py")
            print("   ‚Ä¢ Uses quantization")
            print("   ‚Ä¢ Reduced memory usage")
            
        elif system_capability == "gpu_limited":
            print("1. üéØ RECOMMENDED: Quantized Loading")
            print("   python3 tilde_lightweight.py")
            print("   ‚Ä¢ 8-bit or 4-bit quantization")
            print("   ‚Ä¢ Fits in limited GPU memory")
            
            print("\n2. ‚ö†Ô∏è  Standard Loading (may fail)")
            print("   python3 tilde_model.py")
            print("   ‚Ä¢ May exceed GPU memory")
            
        elif system_capability == "gpu_insufficient":
            print("1. üéØ RECOMMENDED: Cloud GPU")
            print("   ‚Ä¢ Google Colab (free GPU)")
            print("   ‚Ä¢ AWS, Azure, or GCP instances")
            print("   ‚Ä¢ Kaggle notebooks")
            
            print("\n2. ‚ö†Ô∏è  Quantized CPU (very slow)")
            print("   python3 tilde_cpu_friendly.py")
            print("   ‚Ä¢ Extremely slow on CPU")
            print("   ‚Ä¢ Requires 32GB+ RAM")
            
        else:  # cpu_only
            print("1. üéØ RECOMMENDED: Cloud GPU Services")
            print("   ‚Ä¢ Google Colab: https://colab.research.google.com/")
            print("   ‚Ä¢ Kaggle: https://www.kaggle.com/code")
            print("   ‚Ä¢ Hugging Face Spaces (if available)")
            
            print("\n2. ‚ö†Ô∏è  Local CPU (not recommended)")
            print("   python3 tilde_cpu_friendly.py")
            print("   ‚Ä¢ Requires 32GB+ RAM")
            print("   ‚Ä¢ Extremely slow (minutes per response)")
        
        print(f"\nüí° Alternative: Use a smaller model for testing:")
        print("   ‚Ä¢ microsoft/DialoGPT-large")
        print("   ‚Ä¢ TinyLlama/TinyLlama-1.1B-Chat-v1.0")
        print("   ‚Ä¢ microsoft/phi-2")
    
    def cleanup(self):
        """Clean up temporary files."""
        import shutil
        try:
            shutil.rmtree(self.temp_cache)
            print(f"üßπ Cleaned up temporary cache")
        except:
            pass

def main():
    """Main function for minimal demo."""
    print("üåü TildeOpen-30b Minimal Interface")
    print("=" * 50)
    print("üéØ This demo loads only the tokenizer (small download)")
    print("üí° Perfect for exploring the model without 60GB+ download")
    
    model = MinimalTildeModel()
    
    try:
        # Load tokenizer
        if not model.load_tokenizer_only():
            return
        
        # Run capabilities demo
        if model.demo_without_model():
            # Show loading options
            model.show_model_loading_options()
            
            # Interactive demo
            print(f"\n‚ùì Would you like to try the interactive tokenizer demo?")
            response = input("Try interactive demo? (y/N): ").strip().lower()
            
            if response == 'y':
                model.interactive_tokenizer_demo()
        
    finally:
        model.cleanup()

if __name__ == "__main__":
    main()
